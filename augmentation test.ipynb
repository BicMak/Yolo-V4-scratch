{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import lr_scheduler \n",
    "\n",
    "\n",
    "import Custom_dataset\n",
    "import anchor\n",
    "import backbone\n",
    "from CustomAugment import Cutmix\n",
    "from configuration import dataset_config\n",
    "from configuration import dataloader_config\n",
    "from configuration import OptimizerConfig\n",
    "from configuration import augmentation_config\n",
    "from configuration import training_config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set size: 1084, test_set size : 272\n",
      "batch shape: torch.Size([32, 3, 448, 448])\n",
      "bboxes shape: torch.Size([32, 12348, 4])\n",
      "class_labels shape: torch.Size([32, 12348])\n",
      "image shape: torch.Size([32, 3, 448, 448])\n",
      "\n",
      "\n",
      "backbone output : [torch.Size([32, 32, 448, 448]), torch.Size([32, 64, 224, 224]), torch.Size([32, 128, 112, 112]), torch.Size([32, 256, 56, 56]), torch.Size([32, 512, 28, 28]), torch.Size([32, 1024, 14, 14])]\n",
      "P_large_output : torch.Size([32, 512, 14, 14])\n",
      "route_3 interpolation  : torch.Size([32, 256, 28, 28])\n",
      "before route_2  : torch.Size([32, 512, 28, 28])\n",
      "after route_2  : torch.Size([32, 256, 28, 28])\n",
      "torch.Size([32, 256, 28, 28])\n",
      "large : torch.Size([32, 21, 14, 14]), mid : torch.Size([32, 21, 28, 28]), small : torch.Size([32, 21, 56, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i, data in enumerate(train_dataset):\\n    print(f\"\\nüåÄ step : {i}\")\\n    print(f\"bboxes shape: {data[\\'bboxes\\'].shape}\")\\n    print(f\"class_labels shape: {data[\\'class_labels\\'].shape}\")\\n    \\n    result_bboxes, result_labels = image_test.encoder.encoder(data[\\'bboxes\\'], data[\\'class_labels\\'])\\n    \\n    print(f\"result_bboxes shape: {result_bboxes.shape}\")\\n    print(f\"result_labels shape: {result_labels.shape}\")\\n    if i == 5:\\n        break\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated() \n",
    "\n",
    "\n",
    "image_dir = dataset_config.image_dir\n",
    "label_dir = dataset_config.label_dir\n",
    "classes = dataset_config.classes\n",
    "img_size = dataset_config.image_size\n",
    "\n",
    "image_test =Custom_dataset.ListDataset(image_dir,\n",
    "                                       label_dir,\n",
    "                                       classes,\n",
    "                                       transform=augmentation_config.transform)\n",
    "\n",
    "# RabdinSized BBox Safe Crop has box bigger than 1% of image size\n",
    "train_transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=img_size,height=img_size,erosion_rate=0.8),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                            ))\n",
    "\n",
    "final_transform = A.Compose([\n",
    "    A.AdditiveNoise(noise_type=\"gaussian\",\n",
    "                    spatial_mode=\"constant\",\n",
    "                    noise_params={\"mean_range\": (0.0, 0.0), \"std_range\": (0.05, 0.15)}),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n",
    "    A.Affine(translate_percent={'x': (-0.2, 0.2), 'y': (-0.2, 0.2)},  # xÏ∂ï 10~20%, yÏ∂ï -20~20% ÎûúÎç§ Ïù¥Îèô\n",
    "             p=0.3),\n",
    "    A.Affine(rotate = (-20,20), p = 0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                           ))\n",
    "\n",
    "aug_para ={\"alpha\":augmentation_config.alpha,\n",
    "           \"lambda\":augmentation_config.lamda,\n",
    "           \"prob\":augmentation_config.prob}\n",
    "\n",
    "\n",
    "image_test.add_agumentation(train_transform,aug_para,final_transform)\n",
    "\n",
    "set_size = len(image_test)\n",
    "train_dataset_size = int(0.8*set_size)\n",
    "test_dataset_size = set_size-train_dataset_size\n",
    "\n",
    "train_indces, valid_indces = torch.utils.data.random_split(range(set_size), [train_dataset_size, test_dataset_size])\n",
    "print(\"set size: {0}, test_set size : {1}\".format(train_dataset_size,test_dataset_size))\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "valid_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "# This problem is occured in Dater type interupt between array and tensor\n",
    "\n",
    "batch_size = dataloader_config.batch_size\n",
    "num_workers = dataloader_config.num_workers\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=dataloader_config.batch_size,\n",
    "                                               num_workers=dataloader_config.num_workers,\n",
    "                                               shuffle = dataloader_config.shuffle,\n",
    "                                               collate_fn = image_test.collate_fn)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=dataloader_config.batch_size,\n",
    "                                               num_workers=dataloader_config.num_workers,\n",
    "                                               collate_fn = image_test.collate_fn)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "model = backbone.YoloV4Model(num_classes=len(dataset_config.classes),\n",
    "                             to_vector = True)\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(training_config.epochs):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        print(f\"batch shape: {data['image'].shape}\")\n",
    "        print(f\"bboxes shape: {data['bboxes'].shape}\")\n",
    "        print(f\"class_labels shape: {data['class_labels'].shape}\")\n",
    "        print(f\"image shape: {data['image'].shape}\")\n",
    "        print(\"\\n\")\n",
    "        gt_datas = data\n",
    "        output = model(data['image'])\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for i, data in enumerate(train_dataset):\n",
    "    print(f\"\\nüåÄ step : {i}\")\n",
    "    print(f\"bboxes shape: {data['bboxes'].shape}\")\n",
    "    print(f\"class_labels shape: {data['class_labels'].shape}\")\n",
    "    \n",
    "    result_bboxes, result_labels = image_test.encoder.encoder(data['bboxes'], data['class_labels'])\n",
    "    \n",
    "    print(f\"result_bboxes shape: {result_bboxes.shape}\")\n",
    "    print(f\"result_labels shape: {result_labels.shape}\")\n",
    "    if i == 5:\n",
    "        break\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 588, 7])\n",
      "torch.Size([32, 2352, 7])\n",
      "torch.Size([32, 9408, 7])\n",
      "torch.Size([32, 12348, 7])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].shape)\n",
    "print(output[1].shape)\n",
    "print(output[2].shape)\n",
    "result = torch.cat((output[0],output[1],output[2]),dim=1)\n",
    "print(result.shape)\n",
    "#ÎÇ¥Ïùº ÏïÑÏπ®Ïóê ÏùºÏñ¥ÎÇòÏÑú ÌôïÏù∏Ìï¥Î¥êÏïºÎê†Í±∞ -> result ÌÖêÏÑúÌÅ¨Í∏∞ ÌôïÏù∏Ïù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define total batch size \n",
    "total_batch_size = math.ceil(train_dataset_size / dataloader_config.batch_size)  # total batch size for all GPUs, accumulate, and gradient steps\n",
    "\n",
    "nbs = 64  # nominal batch size\n",
    "accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing\n",
    "modified_weight_decay= OptimizerConfig.weight_decay * total_batch_size * accumulate / nbs  # scale weight_decay\n",
    "optimizer = optim.Adam(params = model.parameters(),\n",
    "                       lr=OptimizerConfig.lr0, \n",
    "                       weight_decay=modified_weight_decay, #L2 regularization\n",
    "                       betas=(0.9,0.999) )  # adjust beta1 to momentum\n",
    "\n",
    "\n",
    "# Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "# https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
    "warmup_end_lr =  (1-OptimizerConfig.lrf)/2+OptimizerConfig.lrf  # warmup end learning rate\n",
    "warmup_scheduler = lr_scheduler.LinearLR(optimizer, \n",
    "                                         start_factor=OptimizerConfig.lr0, \n",
    "                                         end_factor= warmup_end_lr,\n",
    "                                         total_iters=training_config.warmup_epochs)\n",
    "\n",
    "# Cosine decay after warmup\n",
    "epochs_cosine = training_config.epochs - training_config.warmup_epochs\n",
    "lf = lambda x: ((1 + math.cos(x * math.pi / epochs_cosine)) / 2) * (1 - OptimizerConfig.lrf) + OptimizerConfig.lrf  # cosine\n",
    "cosine_scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import DetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt shape: torch.Size([32, 12348, 5])\n",
      "result shape: torch.Size([32, 12348, 7])\n",
      "iou shape: torch.Size([32, 12348])\n",
      "c shape: torch.Size([32, 12348])\n",
      "d shape: torch.Size([32, 12348])\n",
      "v shape: torch.Size([32, 12348])\n",
      "v shape: torch.Size([32, 12348])\n",
      "alpha shape: torch.Size([32, 12348])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m DetectionLoss(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset_config\u001b[38;5;241m.\u001b[39mclasses))\n\u001b[1;32m---> 18\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Yolo-V4-scratch\\loss.py:35\u001b[0m, in \u001b[0;36mDetectionLoss.forward\u001b[1;34m(self, preds, targets)\u001b[0m\n\u001b[0;32m     33\u001b[0m obj_gt \u001b[38;5;241m=\u001b[39m cls_targets\n\u001b[0;32m     34\u001b[0m obj_gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(obj_gt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m)) \n\u001b[1;32m---> 35\u001b[0m obj_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_objectness_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 3. Classification Loss\u001b[39;00m\n\u001b[0;32m     39\u001b[0m cls_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_classification_loss(cls_preds, cls_targets)\n",
      "File \u001b[1;32mc:\\Users\\User\\Yolo-V4-scratch\\loss.py:61\u001b[0m, in \u001b[0;36mDetectionLoss.compute_objectness_loss\u001b[1;34m(self, pred_obj, target_obj)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mCompute objectness loss using binary cross-entropy.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m obj_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(target_obj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))   \u001b[38;5;66;03m# GTÏôÄ Îß§Ïπ≠: 1, ÎÇòÎ®∏ÏßÄ: 0\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m obj_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m obj_loss \u001b[38;5;241m=\u001b[39m obj_loss \u001b[38;5;241m*\u001b[39m obj_target  \u001b[38;5;66;03m# Î¨¥ÏãúÎêú Í≤ÉÎì§Ïóê ÎåÄÌï¥ loss Ï§ÑÏûÑ\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj_loss\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3244\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m   3242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "from loss import DetectionLoss\n",
    "\n",
    "pred_loc = result[:,:,:4]\n",
    "pred_obj = result[:,:,4]\n",
    "prod_cls = result[:,:,5:]\n",
    "\n",
    "\n",
    "gt_bboxes = gt_datas['bboxes']\n",
    "gt_class_labels = gt_datas['class_labels']\n",
    "gt_class_labels = gt_class_labels.view(gt_class_labels.shape[0],\n",
    "                                       gt_class_labels.shape[1],\n",
    "                                       1)\n",
    "gt = torch.cat((gt_bboxes,gt_class_labels),dim=2)\n",
    "print(f\"gt shape: {gt.shape}\")\n",
    "print(f\"result shape: {result.shape}\")\n",
    "\n",
    "loss = DetectionLoss(num_classes=len(dataset_config.classes))\n",
    "loss_value = loss.forward(result, gt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
