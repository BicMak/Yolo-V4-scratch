{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.6'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import lr_scheduler \n",
    "\n",
    "\n",
    "import Custom_dataset\n",
    "import anchor\n",
    "import backbone\n",
    "from CustomAugment import Cutmix\n",
    "from configuration import dataset_config\n",
    "from configuration import dataloader_config\n",
    "from configuration import OptimizerConfig\n",
    "from configuration import augmentation_config\n",
    "from configuration import training_config\n",
    "from loss import DetectionLoss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set size: 1084, test_set size : 272\n",
      "batch shape: torch.Size([32, 3, 448, 448])\n",
      "bboxes shape: torch.Size([32, 12348, 4])\n",
      "class_labels shape: torch.Size([32, 12348])\n",
      "image shape: torch.Size([32, 3, 448, 448])\n",
      "\n",
      "\n",
      "backbone output : [torch.Size([32, 32, 448, 448]), torch.Size([32, 64, 224, 224]), torch.Size([32, 128, 112, 112]), torch.Size([32, 256, 56, 56]), torch.Size([32, 512, 28, 28]), torch.Size([32, 1024, 14, 14])]\n",
      "P_large_output : torch.Size([32, 512, 14, 14])\n",
      "route_3 interpolation  : torch.Size([32, 256, 28, 28])\n",
      "before route_2  : torch.Size([32, 512, 28, 28])\n",
      "after route_2  : torch.Size([32, 256, 28, 28])\n",
      "torch.Size([32, 256, 28, 28])\n",
      "large : torch.Size([32, 21, 14, 14]), mid : torch.Size([32, 21, 28, 28]), small : torch.Size([32, 21, 56, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i, data in enumerate(train_dataset):\\n    print(f\"\\nüåÄ step : {i}\")\\n    print(f\"bboxes shape: {data[\\'bboxes\\'].shape}\")\\n    print(f\"class_labels shape: {data[\\'class_labels\\'].shape}\")\\n    \\n    result_bboxes, result_labels = image_test.encoder.encoder(data[\\'bboxes\\'], data[\\'class_labels\\'])\\n    \\n    print(f\"result_bboxes shape: {result_bboxes.shape}\")\\n    print(f\"result_labels shape: {result_labels.shape}\")\\n    if i == 5:\\n        break\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_dir = dataset_config.image_dir\n",
    "label_dir = dataset_config.label_dir\n",
    "classes = dataset_config.classes\n",
    "img_size = dataset_config.image_size\n",
    "\n",
    "image_test =Custom_dataset.ListDataset(image_dir,\n",
    "                                       label_dir,\n",
    "                                       classes,\n",
    "                                       transform=augmentation_config.transform)\n",
    "\n",
    "# RabdinSized BBox Safe Crop has box bigger than 1% of image size\n",
    "train_transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=img_size,height=img_size,erosion_rate=0.8),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                            ))\n",
    "\n",
    "final_transform = A.Compose([\n",
    "    A.AdditiveNoise(noise_type=\"gaussian\",\n",
    "                    spatial_mode=\"constant\",\n",
    "                    noise_params={\"mean_range\": (0.0, 0.0), \"std_range\": (0.05, 0.15)}),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n",
    "    A.Affine(translate_percent={'x': (-0.2, 0.2), 'y': (-0.2, 0.2)},  # xÏ∂ï 10~20%, yÏ∂ï -20~20% ÎûúÎç§ Ïù¥Îèô\n",
    "             p=0.3),\n",
    "    A.Affine(rotate = (-20,20), p = 0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                           ))\n",
    "\n",
    "aug_para ={\"alpha\":augmentation_config.alpha,\n",
    "           \"lambda\":augmentation_config.lamda,\n",
    "           \"prob\":augmentation_config.prob}\n",
    "\n",
    "\n",
    "image_test.add_agumentation(train_transform,aug_para,final_transform)\n",
    "\n",
    "set_size = len(image_test)\n",
    "train_dataset_size = int(0.8*set_size)\n",
    "test_dataset_size = set_size-train_dataset_size\n",
    "\n",
    "train_indces, valid_indces = torch.utils.data.random_split(range(set_size), [train_dataset_size, test_dataset_size])\n",
    "print(\"set size: {0}, test_set size : {1}\".format(train_dataset_size,test_dataset_size))\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "valid_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "# This problem is occured in Dater type interupt between array and tensor\n",
    "\n",
    "batch_size = dataloader_config.batch_size\n",
    "num_workers = dataloader_config.num_workers\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=dataloader_config.batch_size,\n",
    "                                               num_workers=dataloader_config.num_workers,\n",
    "                                               shuffle = dataloader_config.shuffle,\n",
    "                                               collate_fn = image_test.collate_fn)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=dataloader_config.batch_size,\n",
    "                                               num_workers=dataloader_config.num_workers,\n",
    "                                               collate_fn = image_test.collate_fn)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "model = backbone.YoloV4Model(num_classes=len(dataset_config.classes),\n",
    "                             to_vector = True)\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(training_config.epochs):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        print(f\"batch shape: {data['image'].shape}\")\n",
    "        print(f\"bboxes shape: {data['bboxes'].shape}\")\n",
    "        print(f\"class_labels shape: {data['class_labels'].shape}\")\n",
    "        print(f\"image shape: {data['image'].shape}\")\n",
    "        print(\"\\n\")\n",
    "        gt_datas = data\n",
    "        output = model(data['image'])\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "for i, data in enumerate(train_dataset):\n",
    "    print(f\"\\nüåÄ step : {i}\")\n",
    "    print(f\"bboxes shape: {data['bboxes'].shape}\")\n",
    "    print(f\"class_labels shape: {data['class_labels'].shape}\")\n",
    "    \n",
    "    result_bboxes, result_labels = image_test.encoder.encoder(data['bboxes'], data['class_labels'])\n",
    "    \n",
    "    print(f\"result_bboxes shape: {result_bboxes.shape}\")\n",
    "    print(f\"result_labels shape: {result_labels.shape}\")\n",
    "    if i == 5:\n",
    "        break\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 588, 7])\n",
      "torch.Size([32, 2352, 7])\n",
      "torch.Size([32, 9408, 7])\n",
      "torch.Size([32, 12348, 7])\n"
     ]
    }
   ],
   "source": [
    "print(output[0].shape)\n",
    "print(output[1].shape)\n",
    "print(output[2].shape)\n",
    "result = torch.cat((output[0],output[1],output[2]),dim=1)\n",
    "print(result.shape)\n",
    "#ÎÇ¥Ïùº ÏïÑÏπ®Ïóê ÏùºÏñ¥ÎÇòÏÑú ÌôïÏù∏Ìï¥Î¥êÏïºÎê†Í±∞ -> result ÌÖêÏÑúÌÅ¨Í∏∞ ÌôïÏù∏Ïù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define total batch size \n",
    "total_batch_size = math.ceil(train_dataset_size / dataloader_config.batch_size)  # total batch size for all GPUs, accumulate, and gradient steps\n",
    "\n",
    "nbs = 64  # nominal batch size\n",
    "accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing\n",
    "modified_weight_decay= OptimizerConfig.weight_decay * total_batch_size * accumulate / nbs  # scale weight_decay\n",
    "optimizer = optim.Adam(params = model.parameters(),\n",
    "                       lr=OptimizerConfig.lr0, \n",
    "                       weight_decay=modified_weight_decay, #L2 regularization\n",
    "                       betas=(0.9,0.999) )  # adjust beta1 to momentum\n",
    "\n",
    "\n",
    "# Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "# https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n",
    "warmup_end_lr =  (1-OptimizerConfig.lrf)/2+OptimizerConfig.lrf  # warmup end learning rate\n",
    "warmup_scheduler = lr_scheduler.LinearLR(optimizer, \n",
    "                                         start_factor=OptimizerConfig.lr0, \n",
    "                                         end_factor= warmup_end_lr,\n",
    "                                         total_iters=training_config.warmup_epochs)\n",
    "\n",
    "# Cosine decay after warmup\n",
    "epochs_cosine = training_config.epochs - training_config.warmup_epochs\n",
    "lf = lambda x: ((1 + math.cos(x * math.pi / epochs_cosine)) / 2) * (1 - OptimizerConfig.lrf) + OptimizerConfig.lrf  # cosine\n",
    "cosine_scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import DetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt shape: torch.Size([32, 12348, 5])\n",
      "result shape: torch.Size([32, 12348, 7])\n",
      "v shape: torch.Size([32, 12348])\n",
      "result shape: tensor([[ 4.0057, 47.1536,  3.0421,  ...,  3.4164,  4.1620,  3.4402],\n",
      "        [16.7331,  6.5871,  3.8711,  ...,  2.8750,  2.0223,  2.8438],\n",
      "        [ 1.4467,  4.6646, 14.3238,  ...,  1.1110,  2.7488,  1.1261],\n",
      "        ...,\n",
      "        [ 4.0114, 18.0631,  3.6966,  ...,  0.6590,  2.1144,  0.7692],\n",
      "        [ 2.9857,  0.9602,  1.6035,  ...,  0.9217,  1.6492,  1.6313],\n",
      "        [ 1.4019, 13.5575,  0.8710,  ...,  1.8044,  1.1340,  0.5566]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "gt_cls : torch.Size([32, 12348])\n",
      "gt_cls_max : 2\n",
      "gt_cls_min : 0\n",
      "cls_target : torch.Size([32, 12348, 3])\n",
      "target_obj : torch.Size([32, 12348, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "pred_loc = result[:,:,:4]\n",
    "pred_obj = result[:,:,4]\n",
    "prod_cls = result[:,:,5:]\n",
    "\n",
    "\n",
    "gt_bboxes = gt_datas['bboxes']\n",
    "gt_class_labels = gt_datas['class_labels']\n",
    "gt_class_labels = gt_class_labels.view(gt_class_labels.shape[0],\n",
    "                                       gt_class_labels.shape[1],\n",
    "                                       1)\n",
    "gt = torch.cat((gt_bboxes,gt_class_labels),dim=2)\n",
    "print(f\"gt shape: {gt.shape}\")\n",
    "print(f\"result shape: {result.shape}\")\n",
    "\n",
    "loss = DetectionLoss(num_classes=len(dataset_config.classes))\n",
    "loss_value = loss.forward(result, gt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value: {'total': tensor(nan, grad_fn=<AddBackward0>), 'ciou': tensor(nan, grad_fn=<MeanBackward0>), 'obj': tensor(0.1058, grad_fn=<MeanBackward0>), 'cls': tensor(0.6379, grad_fn=<MeanBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss value: {loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
