{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Python\\Python313\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import backbone\n",
    "import anchor\n",
    "import importlib\n",
    "import dataset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set size: 1084, test_set size : 272\n",
      "current index of dataset : 797\n",
      "selected augmentation: mix up\n",
      "MIXUP BBOX CHECK\n",
      "before cut: [0.09333252161741257, 0.3660964071750641, 0.0533333346247673, 0.029914790764451027]\n",
      "before cut: [0.17333415150642395, 0.34188035130500793, 0.04999837279319763, 0.028490375727415085]\n",
      "before cut: [0.23999999463558197, 0.35185232758522034, 0.04999837279319763, 0.029914790764451027]\n",
      "before cut: [0.31333211064338684, 0.34330475330352783, 0.04333211109042168, 0.02421608939766884]\n",
      "before cut: [0.3933337330818176, 0.33903151750564575, 0.04666707292199135, 0.029914790764451027]\n",
      "before cut: [0.4333345592021942, 0.34188035130500793, 0.04333211109042168, 0.029914790764451027]\n",
      "before cut: [0.4766668677330017, 0.3447291851043701, 0.03666585311293602, 0.02421608939766884]\n",
      "before cut: [0.5300002694129944, 0.35042789578437805, 0.04999847337603569, 0.03276361897587776]\n",
      "before cut: [0.753334641456604, 0.3589743673801422, 0.06333435326814651, 0.035612449049949646]\n",
      "before cut: [0.726666271686554, 0.33048394322395325, 0.04666707292199135, 0.027065962553024292]\n",
      "before cut: [0.7966668605804443, 0.3447291851043701, 0.04333231598138809, 0.029914790764451027]\n",
      "before cut: [0.9750003218650818, 0.34045591950416565, 0.04999938979744911, 0.035612449049949646]\n",
      "before cut: [0.07259556651115417, 0.7836407423019409, 0.03811187297105789, 0.1292869597673416]\n",
      "before cut: [0.16515488922595978, 0.15039567649364471, 0.09981808811426163, 0.13192634284496307]\n",
      "before cut: [0.3157893419265747, 0.09498715400695801, 0.08529884368181229, 0.1530342549085617]\n",
      "before cut: [0.8457344174385071, 0.7229548096656799, 0.09981798380613327, 0.16622690856456757]\n",
      "before cut: [0.8312152624130249, 0.3799477815628052, 0.08892988413572311, 0.15039631724357605]\n",
      "--------------------\n",
      "image type in mix augmetation function : float64\n",
      "uint8\n",
      "[0] success: torch.Size([3, 450, 450]), 17 boxes\n",
      "current index of dataset : 1272\n",
      "selected augmentation: 4 mosaic\n",
      "[1] failed with error: 'class_label'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 54\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] success: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m boxes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hellboy_KIM\\Documents\\GitHub\\Yolo-V4-scratch\\dataset.py:80\u001b[0m, in \u001b[0;36mListDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransform:\n\u001b[1;32m---> 80\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmix_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Hellboy_KIM\\Documents\\GitHub\\Yolo-V4-scratch\\dataset.py:125\u001b[0m, in \u001b[0;36mListDataset.mix_augmentation\u001b[1;34m(self, idx_1, auged_set1)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfour_mosaic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mauged_set1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mauged_set2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mauged_set3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mauged_set4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m result_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(result[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hellboy_KIM\\Documents\\GitHub\\Yolo-V4-scratch\\dataset.py:272\u001b[0m, in \u001b[0;36mSimpleMosaic.__call__\u001b[1;34m(self, dataset_lst, force_apply)\u001b[0m\n\u001b[0;32m    269\u001b[0m mosaic_img[y1:y2,x1:x2,:] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[0;32m    273\u001b[0m     x_c, y_c, bw, bh \u001b[38;5;241m=\u001b[39m bbox\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class_label'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] failed with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m[i])\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes :\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_dataset\u001b[38;5;241m.\u001b[39mboxes[i])\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# or continue if you want to scan all\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "image_dir = \"dataset\\\\image\\\\\"\n",
    "label_dir = \"dataset\\\\label\\\\\"\n",
    "classes = {0:'background',1:'NG',2:'OK'}\n",
    "\n",
    "image_test =dataset.ListDataset(image_dir,label_dir,classes,transform=True)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=450,height=450,erosion_rate=0.8),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'] # Specify label argument name(s)\n",
    "                            ))\n",
    "\n",
    "final_transform = A.Compose([\n",
    "    A.AdditiveNoise(noise_type=\"gaussian\",\n",
    "                    spatial_mode=\"constant\",\n",
    "                    noise_params={\"mean_range\": (0.0, 0.0), \"std_range\": (0.05, 0.15)}),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n",
    "    A.Affine(translate_percent={'x': (-0.2, 0.2), 'y': (-0.2, 0.2)},  # x축 10~20%, y축 -20~20% 랜덤 이동\n",
    "             p=0.3),\n",
    "    A.Affine(rotate = (-20,20), p = 0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                           ))\n",
    "\n",
    "aug_para ={\"alpha\":0.7,\"lambda\":0.7,\"prob\":0.5}\n",
    "\n",
    "\n",
    "image_test.add_agumentation(train_transform,aug_para,final_transform)\n",
    "\n",
    "set_size = len(image_test)\n",
    "train_dataset_size = int(0.8*set_size)\n",
    "test_dataset_size = set_size-train_dataset_size\n",
    "\n",
    "train_indces, valid_indces = torch.utils.data.random_split(range(set_size), [train_dataset_size, test_dataset_size])\n",
    "print(\"set size: {0}, test_set size : {1}\".format(train_dataset_size,test_dataset_size))\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "valid_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "\n",
    "#지금 문제는 torch.float32의 이미지 자료형이 \n",
    "#엔코더에 입력변수로 이미지로 넣었을때 np.array하고 torch.tensor \n",
    "# 간에 충돌이 일어나면서 발생하고 있음\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    print(\"current index of dataset : {0}\".format(train_indces[i]))\n",
    "    try:\n",
    "        data = train_dataset[i]\n",
    "        print(f\"[{i}] success: {data['image'].shape}, {len(data['bboxes'])} boxes\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] failed with error: {e}\")\n",
    "        print(\"labels:\", train_dataset.labels[i])\n",
    "        print(\"boxes :\", train_dataset.boxes[i])\n",
    "        break  # or continue if you want to scan all\n",
    "\n",
    "batch_size = 200\n",
    "num_workers = 1\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               collate_fn = image_test.collate_fn)\n",
    "\n",
    "loader_iter = iter(train_dataloader)\n",
    "batch = next(loader_iter)\n",
    "\n",
    "image, loc, class_target = batch\n",
    "print(image.shape, loc.shape, class_target.shape)\n",
    "\n",
    "'''\n",
    "print(result[\"image\"].shape)\n",
    "print(result[\"image\"].dtype)\n",
    "print(result[\"bboxes\"])\n",
    "print(result[\"class_label\"])\n",
    "\n",
    "img = result[\"image\"]\n",
    "result_label = result[\"class_label\"]\n",
    "\n",
    "for i, box in enumerate(result[\"bboxes\"]):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(anchor)\n",
    "\n",
    "anchoring = anchor.DataEncoder((450,450),classes=[0,1])\n",
    "\n",
    "\n",
    "test_out,test_cls = anchoring.encoder(gt_boxes,gt_classes)\n",
    "\n",
    "print(test_cls)\n",
    "\n",
    "unique, counts = test_cls.unique(return_counts=True)\n",
    "for u, c in zip(unique, counts):  \n",
    "    print(f\"클래스 {int(u)}: {int(c)}개\")\n",
    "\n",
    "\n",
    "print(13*13*3,26*26*3,52*52*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "image_dir = \"dataset\\\\image\\\\\"\n",
    "label_dir = \"dataset\\\\label\\\\\"\n",
    "classes = {0:'background',1:'NG',2:'OK'}\n",
    "\n",
    "image_test =dataset.ListDataset(image_dir,label_dir,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "image,boxes, label=image_test[0]\n",
    "print(len(image_test))\n",
    "print(label)\n",
    "print(image.shape)\n",
    "print(boxes)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_lst = ['NG','OK']\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=450,height=450,erosion_rate=0.8),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'] # Specify label argument name(s)\n",
    "                           ))\n",
    "\n",
    "augmented = train_transform(image=image,bboxes=boxes,class_labels=label)\n",
    "\n",
    "img = augmented['image']\n",
    "result_label = augmented['class_labels']\n",
    "\n",
    "print(img.dtype)\n",
    "for i, box in enumerate(augmented['bboxes']):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MixUp:\n",
    "    def __init__(self, alpha=0.7, prob=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, datsset1, dataset2):\n",
    "        img1 = np.array(datsset1['image']) / 255\n",
    "        img2 = np.array(dataset2['image']) / 255\n",
    "        mixed_img = (self.alpha * img1 + (1-self.alpha) * img2)\n",
    "        mixed_boxes = datsset1['bboxes'] + dataset2['bboxes'] \n",
    "        mixed_labels = datsset1['class_labels'] + dataset2['class_labels']\n",
    "\n",
    "        return mixed_img, mixed_boxes, mixed_labels\n",
    "\n",
    "image_A,boxes_A,label_A=image_test[0]\n",
    "image_B,boxes_B,label_B=image_test[100]\n",
    "augmentedA = train_transform(image=image_A,bboxes=boxes_A,class_labels=label_A)\n",
    "augmentedB = train_transform(image=image_B,bboxes=boxes_B,class_labels=label_B)\n",
    "\n",
    "aug2 = MixUp(alpha=0.5, prob=1)\n",
    "m_1,m_2,m_3 = aug2(augmentedA,augmentedB)\n",
    "\n",
    "img = m_1\n",
    "result_label = m_3\n",
    "\n",
    "for i, box in enumerate(m_2):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_transform(transform, dataset, indices):\n",
    "    \"\"\"\n",
    "    transform: Albumentations transform 객체 (예: train_transform)\n",
    "    dataset: [(image, boxes, labels), ...] 형태의 리스트 or 커스텀 Dataset\n",
    "    indices: 4개의 인덱스를 가진 리스트 (예: [0, 100, 10, 75])\n",
    "\n",
    "    return: list of transformed outputs [(image, boxes, labels), ...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx in indices:\n",
    "        image, boxes, labels = dataset[idx]\n",
    "        augmented = transform(image=image, bboxes=boxes, class_labels=labels)\n",
    "        results.append((augmented['image'], augmented['bboxes'], augmented['class_labels']))\n",
    "    return results\n",
    "\n",
    "augmented_outputs = apply_transform(train_transform,image_test,[10,20,30,40])\n",
    "images = [out[0] for out in augmented_outputs]\n",
    "bboxes = [out[1] for out in augmented_outputs]\n",
    "labels = [out[2] for out in augmented_outputs]\n",
    "\n",
    "Mosic_test = dataset.SimpleMosaic(225,True)\n",
    "m_1,m_2,m_3 = Mosic_test(image=images, bboxes=bboxes, class_labels=labels)\n",
    "\n",
    "img = m_1\n",
    "result_label = m_3\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "for i, box in enumerate(m_2):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "importlib.reload(dataset)\n",
    "\n",
    "image_A,boxes_A,label_A=image_test[0]\n",
    "image_B,boxes_B,label_B=image_test[99]\n",
    "augmentedA = train_transform(image=image_A,bboxes=boxes_A,class_labels=label_A)\n",
    "augmentedB = train_transform(image=image_B,bboxes=boxes_B,class_labels=label_B)\n",
    "\n",
    "aug2 = dataset.Cutmix(lamda=0.5, prob=1)\n",
    "m_1,m_2,m_3 = aug2(augmentedA,augmentedB)\n",
    "\n",
    "img = m_1\n",
    "result_label = m_3\n",
    "\n",
    "for i, box in enumerate(m_2):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
