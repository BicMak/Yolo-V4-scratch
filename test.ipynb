{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import backbone\n",
    "import anchor\n",
    "import importlib\n",
    "import dataset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BboxParams.__init__() got an unexpected keyword argument 'clip_boxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      5\u001b[0m classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNG\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      7\u001b[0m image_test \u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mListDataset(image_dir,label_dir,classes,transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     10\u001b[0m     A\u001b[38;5;241m.\u001b[39mRandomSizedBBoxSafeCrop(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m450\u001b[39m,height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m450\u001b[39m,erosion_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m),\n\u001b[0;32m     11\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m     12\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[1;32m---> 13\u001b[0m ], bbox_params\u001b[38;5;241m=\u001b[39m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBboxParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify input format\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify label argument name(s)\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mclip_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m final_transform \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     19\u001b[0m     A\u001b[38;5;241m.\u001b[39mAdditiveNoise(noise_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m                     spatial_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m                            clip_boxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     31\u001b[0m                            ))\n\u001b[0;32m     33\u001b[0m aug_para \u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.5\u001b[39m}\n",
      "\u001b[1;31mTypeError\u001b[0m: BboxParams.__init__() got an unexpected keyword argument 'clip_boxes'"
     ]
    }
   ],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "image_dir = \"dataset\\\\image\\\\\"\n",
    "label_dir = \"dataset\\\\label\\\\\"\n",
    "classes = {0:'background',1:'NG',2:'OK'}\n",
    "\n",
    "image_test =dataset.ListDataset(image_dir,label_dir,classes,transform=True)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=450,height=450,erosion_rate=0.8),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                           clip_boxes=True\n",
    "                            ))\n",
    "\n",
    "final_transform = A.Compose([\n",
    "    A.AdditiveNoise(noise_type=\"gaussian\",\n",
    "                    spatial_mode=\"constant\",\n",
    "                    noise_params={\"mean_range\": (0.0, 0.0), \"std_range\": (0.05, 0.15)}),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),\n",
    "    A.Affine(translate_percent={'x': (-0.2, 0.2), 'y': (-0.2, 0.2)},  # x축 10~20%, y축 -20~20% 랜덤 이동\n",
    "             p=0.3),\n",
    "    A.Affine(rotate = (-20,20), p = 0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'], # Specify label argument name(s)\n",
    "                           clip_boxes=True\n",
    "                           ))\n",
    "\n",
    "aug_para ={\"alpha\":0.7,\"lambda\":0.7,\"prob\":0.5}\n",
    "\n",
    "\n",
    "image_test.add_agumentation(train_transform,aug_para,final_transform)\n",
    "\n",
    "set_size = len(image_test)\n",
    "train_dataset_size = int(0.8*set_size)\n",
    "test_dataset_size = set_size-train_dataset_size\n",
    "\n",
    "train_indces, valid_indces = torch.utils.data.random_split(range(set_size), [train_dataset_size, test_dataset_size])\n",
    "print(\"set size: {0}, test_set size : {1}\".format(train_dataset_size,test_dataset_size))\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "valid_dataset = torch.utils.data.Subset(image_test, train_indces)\n",
    "\n",
    "# This problem is occured in Dater type interupt between array and tensor\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    print(\"current index of dataset : {0}\".format(train_indces[i]))\n",
    "    try:\n",
    "        data = train_dataset[i]\n",
    "        print(f\"[{i}] success: {data['image'].shape}, {len(data['bboxes'])} boxes\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] failed with error: {e}\")\n",
    "        break  # or continue if you want to scan all\n",
    "    print(\" \")\n",
    "\n",
    "batch_size = 200\n",
    "num_workers = 1\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               collate_fn = image_test.collate_fn)\n",
    "\n",
    "#loader_iter = iter(train_dataloader)\n",
    "#batch = next(loader_iter)\n",
    "\n",
    "#image, loc, class_target = batch\n",
    "#print(image.shape, loc.shape, class_target.shape)\n",
    "\n",
    "'''\n",
    "print(result[\"image\"].shape)\n",
    "print(result[\"image\"].dtype)\n",
    "print(result[\"bboxes\"])\n",
    "print(result[\"class_label\"])\n",
    "\n",
    "img = result[\"image\"]\n",
    "result_label = result[\"class_label\"]\n",
    "\n",
    "for i, box in enumerate(result[\"bboxes\"]):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(anchor)\n",
    "\n",
    "anchoring = anchor.DataEncoder((450,450),classes=[0,1])\n",
    "\n",
    "\n",
    "test_out,test_cls = anchoring.encoder(gt_boxes,gt_classes)\n",
    "\n",
    "print(test_cls)\n",
    "\n",
    "unique, counts = test_cls.unique(return_counts=True)\n",
    "for u, c in zip(unique, counts):  \n",
    "    print(f\"클래스 {int(u)}: {int(c)}개\")\n",
    "\n",
    "\n",
    "print(13*13*3,26*26*3,52*52*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "\n",
    "image_dir = \"dataset\\\\image\\\\\"\n",
    "label_dir = \"dataset\\\\label\\\\\"\n",
    "classes = {0:'background',1:'NG',2:'OK'}\n",
    "\n",
    "image_test =dataset.ListDataset(image_dir,label_dir,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "image,boxes, label=image_test[0]\n",
    "print(len(image_test))\n",
    "print(label)\n",
    "print(image.shape)\n",
    "print(boxes)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "label_lst = ['NG','OK']\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=450,height=450,erosion_rate=0.8),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', # Specify input format\n",
    "                           label_fields=['class_labels'] # Specify label argument name(s)\n",
    "                           ))\n",
    "\n",
    "augmented = train_transform(image=image,bboxes=boxes,class_labels=label)\n",
    "\n",
    "img = augmented['image']\n",
    "result_label = augmented['class_labels']\n",
    "\n",
    "print(img.dtype)\n",
    "for i, box in enumerate(augmented['bboxes']):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MixUp:\n",
    "    def __init__(self, alpha=0.7, prob=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, datsset1, dataset2):\n",
    "        img1 = np.array(datsset1['image']) / 255\n",
    "        img2 = np.array(dataset2['image']) / 255\n",
    "        mixed_img = (self.alpha * img1 + (1-self.alpha) * img2)\n",
    "        mixed_boxes = datsset1['bboxes'] + dataset2['bboxes'] \n",
    "        mixed_labels = datsset1['class_labels'] + dataset2['class_labels']\n",
    "\n",
    "        return mixed_img, mixed_boxes, mixed_labels\n",
    "\n",
    "image_A,boxes_A,label_A=image_test[0]\n",
    "image_B,boxes_B,label_B=image_test[100]\n",
    "augmentedA = train_transform(image=image_A,bboxes=boxes_A,class_labels=label_A)\n",
    "augmentedB = train_transform(image=image_B,bboxes=boxes_B,class_labels=label_B)\n",
    "\n",
    "aug2 = MixUp(alpha=0.5, prob=1)\n",
    "m_1,m_2,m_3 = aug2(augmentedA,augmentedB)\n",
    "\n",
    "img = m_1\n",
    "result_label = m_3\n",
    "\n",
    "for i, box in enumerate(m_2):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_transform(transform, dataset, indices):\n",
    "    \"\"\"\n",
    "    transform: Albumentations transform 객체 (예: train_transform)\n",
    "    dataset: [(image, boxes, labels), ...] 형태의 리스트 or 커스텀 Dataset\n",
    "    indices: 4개의 인덱스를 가진 리스트 (예: [0, 100, 10, 75])\n",
    "\n",
    "    return: list of transformed outputs [(image, boxes, labels), ...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx in indices:\n",
    "        image, boxes, labels = dataset[idx]\n",
    "        augmented = transform(image=image, bboxes=boxes, class_labels=labels)\n",
    "        results.append((augmented['image'], augmented['bboxes'], augmented['class_labels']))\n",
    "    return results\n",
    "\n",
    "augmented_outputs = apply_transform(train_transform,image_test,[10,20,30,40])\n",
    "images = [out[0] for out in augmented_outputs]\n",
    "bboxes = [out[1] for out in augmented_outputs]\n",
    "labels = [out[2] for out in augmented_outputs]\n",
    "\n",
    "Mosic_test = dataset.SimpleMosaic(225,True)\n",
    "m_1,m_2,m_3 = Mosic_test(image=images, bboxes=bboxes, class_labels=labels)\n",
    "\n",
    "img = m_1\n",
    "result_label = m_3\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "for i, box in enumerate(m_2):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "importlib.reload(dataset)\n",
    "\n",
    "image_A,boxes_A,label_A=image_test[0]\n",
    "image_B,boxes_B,label_B=image_test[99]\n",
    "augmentedA = train_transform(image=image_A,bboxes=boxes_A,class_labels=label_A)\n",
    "augmentedB = train_transform(image=image_B,bboxes=boxes_B,class_labels=label_B)\n",
    "\n",
    "aug2 = dataset.Cutmix(lamda=0.5, prob=1)\n",
    "m_1,m_2,m_3 = aug2(augmentedA,augmentedB)\n",
    "\n",
    "img = m_1\n",
    "result_label = m_3\n",
    "\n",
    "for i, box in enumerate(m_2):\n",
    "    x_center, y_center, w, h = box\n",
    "    x1 = int((x_center - w/2) * 450)\n",
    "    y1 = int((y_center - h/2) * 450)\n",
    "    x2 = int((x_center + w/2) * 450)\n",
    "    y2 = int((y_center + h/2) * 450)\n",
    "    if result_label[i] == 1:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(255, 0, 0), thickness=2)\n",
    "    else:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "plt.imshow(img)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
